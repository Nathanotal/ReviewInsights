{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_12340\\3392750417.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[\"index\"] = df2.index # Not fantastic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('companyReviews.csv', encoding='utf8')\n",
    "df2 = df[df['review'].notna()]\n",
    "df2[\"index\"] = df2.index # Not fantastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_udpipe\n",
    "spacy_udpipe.download(\"sv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the code from words with limited information and lemmatize\n",
    "from langdetect import detect\n",
    "\n",
    "def initClean(df):\n",
    "    removeWords = [\"!\", \".\", \",\", \"\\r\", \"\\n\", \"-\", \"'\", '\"', \"^\", \"(\", \")\", \"’\", \":\", \";\", \"+\", \"?\"]\n",
    "    with open(\"Stoppord.txt\", encoding=\"utf-8\") as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            removeWords.append(line.strip())\n",
    "    nlp = spacy_udpipe.load(\"sv\")\n",
    "    return removeWords, nlp\n",
    "\n",
    "removeWords, nlp = initClean(df2)\n",
    "\n",
    "def cleanReview(review):\n",
    "    try:\n",
    "        lang = detect(review)\n",
    "    except:\n",
    "        lang = \"ERROR\"\n",
    "        # print(\"Error\", review)\n",
    "        \n",
    "    if lang != \"sv\":\n",
    "        return \"WRONG_LANGUAGE\"\n",
    "    else: # Clean the text, Lemmatize, remove stopwords\n",
    "        doc = nlp(review)\n",
    "        newText = \"\"\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() in removeWords:\n",
    "                pass\n",
    "            else:\n",
    "                newText += token.lemma_.lower() + \" \"\n",
    "        if len(newText) == 0:\n",
    "            return \"WRONG_LANGUAGE\"\n",
    "        newText = newText[:-1]\n",
    "        newText = newText.strip()\n",
    "        return newText\n",
    "    \n",
    "df3 = pd.DataFrame()\n",
    "df3[\"review\"] = df2[\"review\"].apply(cleanReview) # barf\n",
    "df3['grade'] = df2['grade'].astype(int)\n",
    "df3['company'] = df2['company'].astype(str)\n",
    "df3 = df3[df3['review'] != \"WRONG_LANGUAGE\"]\n",
    "df3 = df3[df3['review'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import umap.umap_ as umap\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_features=10000, ngram_range=(1, 2))\n",
    "vz = vectorizer.fit_transform(df3['review'])\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=50)\n",
    "svd_tfidf = svd_model.fit_transform(vz)\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2)\n",
    "umap_tfidf = umap_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "eps = 1 # how close points should be to each other to be considered a part of a cluster, e.g 2.5\n",
    "min_samples = 20 # the minimum number of points to form a dense region, e.g 15\n",
    "dbscan = DBSCAN(eps=eps,min_samples=min_samples)\n",
    "dbscan_model = dbscan.fit(umap_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import numpy as np\n",
    "import lda\n",
    "\n",
    "# create document term matrix\n",
    "min_df = 4 # minimum required occurences of a word, e.g 4\n",
    "max_features = 10000 # max number of unique words, e.g 10000\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=min_df, max_features=max_features, ngram_range=(1, 2)) # unigram & bigram\n",
    "X = vectorizer.fit_transform(df3[\"review\"])\n",
    "\n",
    "\n",
    "# build LDA model\n",
    "n_topics = 25 # pick the number of topics, e.g 5\n",
    "n_iter = 2000 # number of learning iterations, e.g 2000\n",
    "\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(X) # X is document term matrix\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_  # get the topic words\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"wordCount\"] = df3[\"review\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv(\"processed.csv\", encoding=\"utf-8\")\n",
    "# df3 = pd.read_csv('processed.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "# VISUALIZATION\n",
    "def prepareLDAData(lda_model, lda_df, vectorizer):\n",
    "    data = {\n",
    "        'vocab': vocab,\n",
    "        'doc_topic_dists': lda_model.doc_topic_,\n",
    "        'doc_lengths': list(lda_df['wordCount']),\n",
    "        'term_frequency':vectorizer.vocabulary_,\n",
    "        'topic_term_dists': lda_model.components_\n",
    "    } \n",
    "    return data\n",
    "\n",
    "# load the pre-prepared pyLDAvis data\n",
    "lda_data = prepareLDAData(lda_model=lda_model, lda_df=df3, vectorizer=vectorizer)\n",
    "prepared_data = pyLDAvis.prepare(**lda_data)\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def classifier(review=\"hemsidan går att läsa men de är inte bra på att skicka paket. Min stol kom aldrig fram\"):\n",
    "    # topicNames = [\"Seriöshet\", \"Kundtjänst (generisk)\", \"Lager\", \"Betyg(noise)\", \"Returhantering\", \"Kundtjänst (tel)\", \"Leverans/Paket\", \"Bok/Jul (noise)\", \"Leveranstid\", \"Installation\", \"Möbler (noise)\", \"Kläder (noise)\", \"Fraktkostnad\", \"Betalning\", \"Linser + rabatt (noise)\", \"Service (noise)\", \"problem\", \"Lev/betal-alternativ\", \"Hemsida (sök)\", \"Hemsida (tydlighet)\", \"Leverans/sortiment\", \"Leveranstid 2\", \"Leverans(fungera)\", \"Frakt (noise)\", \"Verkstad/Service\"] # These have to be manually changed by a human! (for now)\n",
    "    cleanR = review # cleanReview(review)\n",
    "    vector = vectorizer.transform([cleanR])\n",
    "    v = vector.toarray()\n",
    "    \n",
    "    topic_probs = lda_model.transform(v)\n",
    "    results = []\n",
    "    for i, t_prob in enumerate(topic_probs[0]):\n",
    "        if t_prob > 0.10:\n",
    "            results.append([i, t_prob])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classifierFromX(topicProbs):\n",
    "    n = 0\n",
    "    results = []\n",
    "    for reviewTopics in topicProbs:\n",
    "        topicP = [0 for i in range(n_topics)]\n",
    "        for i, t_prob in enumerate(reviewTopics):\n",
    "            if t_prob > 0.10: # Only keep probabilities where we are kind of sure\n",
    "                topicP[i] = t_prob\n",
    "        results.append(topicP)\n",
    "        n += 1\n",
    "        if n == 4:\n",
    "            break\n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifierFromX(X_topics)\n",
    "df = pd.DataFrame(results)\n",
    "df[\"index\"] = df.index # Not fantastic\n",
    "reviewInfoDf = pd.concat([df3, df])\n",
    "reviewInfoDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewInfoDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main calculation\n",
    "companyScores = {}\n",
    "reviewCount = 0\n",
    "reviewTopicDist = [0 for i in range(n_topics)]\n",
    "previousCompany = reviewInfoDf[\"company\"].iloc[0]\n",
    "n = 0\n",
    "for index, row in reviewInfoDf.iterrows():\n",
    "    company = row[\"company\"]\n",
    "    if company == previousCompany:\n",
    "        reviewCount += 1\n",
    "    else:\n",
    "        companyScores[previousCompany] = [prob/reviewCount for prob in reviewTopicDist]\n",
    "        previousCompany = company\n",
    "        reviewCount = 1\n",
    "        \n",
    "    \n",
    "    print(row.iloc[:25])\n",
    "        \n",
    "    \n",
    "    \n",
    "    n+=1\n",
    "    if n == 4:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f23ea733dd6dc61661bd20a7dd204ea98974fdcb2caa62fa98a2eca2a23377b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
